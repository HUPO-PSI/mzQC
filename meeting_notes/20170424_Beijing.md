# HUPO-PSI Meeting Beijing 24-26 April 2017

## Attendants

- Dave Tabb (<dtabb@sun.ac.za>); chair
- Mathias Walzer (<walzer@ebi.ac.uk>); co-chair
- Reza Salek (<reza.salek@ebi.ac.uk>); MIAPE coordinator
- Weimin Zhu (<wmzhuworld@gmail.com>); MIAPE coordinator
- Martin Eisenacher (<martin.eisenacher@rub.de>); ontology coordinator
- Wout Bittremieux (<wout.bittremieux@uantwerpen.be>); secretary

- Howard Choi
- Jinmeng Jia (<jiajmeurida@163.com>)
- Jennifer Polson (<j.s.polson@gmail.com>)
- Ryan Smith (<r.j.smith@qmul.ac.uk>)
- Others (*please complete*): Bo Wen Chung, Lumin Liang, Jianming, Jen Xia, An Ming, Ning

---

## Introduction: How to contribute to the working group?

Mathias gave an introduction on the structure of the [GitHub repository](https://github.com/HUPO-PSI/qcML-development) as our main communication platform.

An overview of the different folders:

- [`legacy`](https://github.com/HUPO-PSI/qcML-development/tree/master/legacy): Backup of the old specification.
- [`cv`](https://github.com/HUPO-PSI/qcML-development/tree/master/cv): Controlled vocabulary to unambiguously define the QC metrics. This is a flexible way to add new terms without having to change the XML schema.
- [`schema`](https://github.com/HUPO-PSI/qcML-development/tree/master/schema): The XML schema including examples.
- [`meeting_notes`](https://github.com/HUPO-PSI/qcML-development/tree/master/meeting_notes): Newly added folder to store our meeting notes.

---

## XML format specification

The following changes to the XML schema were discussed.

Mathias will compile a prototype qcML file containing the changes presented below. This will be available on GitHub and feedback will be requested on the mailing list.

### JSON metrics representation

We need to have a flexible way to represent complex QC metrics. Metrics can for example consist of a single value, a k-tuple of values (e.g. quartiles, deciles, ...), or a table. To accommodate all these different use cases we will use a content element which can contain free text and a JSON representation will be employed to store multiple values. A close interaction with the CV will be mandatory to explicitly define the format of this field in all possible cases, we don't want to allow any arbitrary expressions.

We briefly discussed whether such a free field is really necessary, and that instead we can explicitly define all complex QC metrics. The consensus seems to be that the flexibility and ease-of-use advantages of the JSON representation outweigh the
disadvantages.

#### Advantages:

- Very flexible to represent any type of content.
- JSON is machine-readable with extensive library support as well as human-interpretable.

#### Disadvantages:

- It's not possible to use XML validators to directly check the validity of the JSON field, instead the semantic validation will need to be implemented externally.
- Base64 might be more useful (concise) for long lists of values.

#### Potential validation strategies:

- List/tuples of a fixed length: Check whether the length is correct.
- Quartiles/deciles/...: Check whether the values sum to one.

#### How will specific types of values be represented?

- Single values will be represented by the classical approach with the value specified as an XML attribute and its unit implicitly defined in the CV.
- Lists of values will be represented by a named JSON dictionary as follows: `{unit_cv_ref:[val1,val2,...]}`. The key contains a CV accession specifying the unit, followed by a list of the values.
- Tables can be represented by coordinated column lists (as above) with each row in the table formed by the items at the same index in each of the lists.
- Long lists or tables (for example a value for every scan in a run) should preferentially be stored as Base64 strings, which is more concise than a JSON-representation of such a long list.

#### Advanced reporting

The free content element also enables advanced reporting using a qcML file, for example by directly including figures.
This isn't the core use case of the qcML format, as the historical PSI philosophy is to use the XML-based formats only for data storage and communication and to outsource the analysis to downstream software tools.
However, advanced reporting functionality is a hard requirement for some tools, such as OpenMS. The free content element covers this functionality as well as it can be used to store any kind of general information.

### Referencing to external (qcML) files

We can specify QC information at 2 different levels:

- Run: Information pertaining to a single run.
- Set: Information pertaining to multiple runs. For example: the difference between theoretical versus measured spiked-in proteins over multiple runs (fixed number of runs), minimum distance to other runs for outlier detection (dynamic number of runs).

We have to be careful with the definition of a "run". For example, in MALDI profiling individual raw files are generated for each spot, while the qcML file would be generated for the whole plate. Generally speaking, a run refers to a single LC or GC separation or one MALDI plate.
This is an important consideration for metabolomics support as well.

How can we specify external files in sets? At the moment this is done implicitly by combining all the runs relevant for a set in a new qcML file. In some situations this might not be feasible, for example when generating QC information at a repository-wide scale for PRIDE. Can we use links to external files instead?

Furthermore, we also need to be able to link to external information in other PSI standards such as the spectra and the identifications. In particular, when validating a qcML file a distinction must be made between ID-free and ID-based metrics. If ID-based metrics are reported then a reference to the identification file must be present for the qcML file to be valid. This will be specified in the file provenance section (see below).

### Metadata to provide (file) provenance

File provenance, i.e. knowing the history of the files leading up to the qcML generation, is important to be able to replicate the analysis. Therefore, the spectrum file that was used to generated the qcML file should be clearly specified in a metadata segment. It is important to explicitly specify the source file, as for example spectra can differ between raw files and mzML files due to peak picking, etc., but also to specify the full conversion history leading up to this file. This is not limited to PSI standards only but can be any kind of file, for example IDPicker input files for QuaMeter.

Necessary information that should be included is the file name, an uri (a file path or a PRIDE/MassIVE/ProteomeXchange identifier), and a checksum (MD5 or SHA-1, based on how this is done for other formats).

Other information that should be recorded is the software that has been used to generate the qcML file including the version.

Additionally, the instrument settings that were used to generate the data are useful information for downstream analysis as well. However, it's not straightforward to define what should or shouldn't be included as there are numerous settings to consider and they lead to very subtle differences in the data that can't be derived from the QC metrics. Importantly, highly vendor-specific metrics shouldn't be included in the format definition directly. It is important to keep the distinction between the general format definition, which has to be applicable to all cases, and individual tools, which can focus on specific use cases. See also for example ThermoRawMetaDump in ProteoWizard which exports instrument-specific configuration settings.

### Small schema changes


- **Rename `QualityParameter` to `QualityMetric`:** `QualityParameter` is a legacy name due to the inheritance of the general PSI `cvParameter` element, but this name can be confusing for novel qcML users. Renaming it to `QualityMetric` is a simple cosmetic change which avoids this confusion.
- **Removal of the `Threshold` element:** The `Threshold` element remains from the legacy qcML specification and such functionality was explicitly requested during last year's meeting in Ghent. However, the current implementation was deemed suboptimal: its usage was unclear and limited as only univariate thresholding decisions could be specified. Therefore, the `Threshold` element and the related flag attribute are removed from the XML specification for the moment. We will revisit how to store this kind of information at a later time, for example to include it in a 1.1 version. Ideally this should then be specified at the run level instead of at the level of individual QC metrics.

## Controlled vocabulary

### CV structure

We can create a hierarchy based on multiple distinctions: metrics for single versus multiple runs, the type of value (single value, tuple, list), whether the metrics are ID-free, ID-based, or are related to quantification, ...
However, an overly complex hierarchy will discourage new developers from adding their metrics to the ontology. Instead a rather flat hierarchy with metrics grouped per specific tools seems advisable. In contrast, we need to be watchful that this won't result in a cluttered mess and that identical metrics across tools can be linked. This last consideration can be done using synonyms in the CV (cfr. the MVH score for Myrimatch and Pepitome in the PSI-MS CV).

To make sure that the procedure to add new terms to the CV is clear good documentation is essential! This is a highly important point as downstream analysis tools will crucially depend on the information represented by the CV. Similarly, the semantic validation of the JSON free text will depend on the CV as well.

We need to ensure a good process exists for this task and that it doesn't only depend on Gerhard as our ontology coordinator. We can later on decide whether we will stick to a separate CV or whether it will be merged into the PSI-MS CV.

Martin showed how OboEdit can be used to easily compile and visualize the CV.

### CV extension

We need a careful definition of what makes a metric, including how to compute it. For example, we often don't know what exactly goes on with the NIST metrics.

The list of metrics that was compiled at last year's meeting in Ghent will have to be converted to the CV.
Furthermore we need to evaluate the current CV terms and possibly rename them where necessary.

## MIAPE-QC

Jinmeng presented her work on minimal reporting requirements, including efforts to unite requirements across different MIAPE documents. Jinmeng will update her list based on the feedback that was provided, which will then be communicated. We discussed some of the current QC tools.

An important consideration is that the barrier to providing MIAPE-QC compliant information should be as low as possible to encourage people to do this. It can be considered more of a manual with recommendations on how to do QC than strict guidelines. Keep in mind that a MIAPE document is intended to function as a human-readable check list instead of being a formally defined document.

Some metrics that were identified as the bare minimum of what should be reported are as follows:

- Accession of the file (where it can be downloaded), the file name, its checksum
- A short description of where it is situated in the experimental hierarchy
- The experiment start time
- The number of MS1 scans
- The number of MS2 scans
- The identification performance

These minimal requirements can be directly generated from the raw files without having to use advanced tools.

Experimental design is a highly important piece of information which is currently still missing. The relationship between the experimental design and the files should be specified. We can refer to MIAPE-Quant to see how it is done there.

## Potential qcML use cases

We have identified several important use cases that the qcML format should definitely support:

- **Metabolomics:** Reza gave a nice introduction on how QC in metabolomics is typically conducted (blanks, pooled QC, internal standards). It is important to include metabolomics researchers to bridge the gap between proteomics and metabolomics. Specific points of attention for the metabolomics use case is to make sure we can support both LC-MS as well as GC-MS metrics. On an unrelated note, similarly MALDI metrics need to be supported as well.
- **Clinical testing laboratories:** To answer the needs of the clinical testing laboratories the QC procedures need to super robust.
- **Inclusion of QC metrics with a publication:** The advantage for the authors of the publication will be the defensibility of their data.
- **QC in repositories:** We need to provide an incentive for people to add QC information to their data submissions to repositories. At the moment this is mostly altruistic as people benefitting from this information are different from the people providing it (data parasite).

#### Which systems will function as qcML prototypes?

- QuaMeter ID-free: Dave will get a new graduate student to work on QuaMeter.
- OpenMS: Mathias has added additional NIST-inspired metrics to OpenMS.

---

## Short remarks

- Do we need to support virtualization efforts such as Docker? No.
- Do we need a separate XML element for spectrum-level QC metrics? No, not explicitly, as one wouldn't generate a qcML file for a single spectrum. We can represent this instead as a table of spectrum identifiers (NativeID) and the corresponding metrics.
An example of a spectrum-level metric is the spectral quality score derived by ScanRanker.
- As mentioned previously a good documentation is crucial for a successful adoption of the qcML format. Besides the standard PSI documentation we will provide "TL;DR" directions for developers.

---

## Monthly teleconferences

We will schedule monthly teleconferences to discuss our progress on the **third Friday of the month at 9:00 GMT**. A reminder before each teleconference will be posted on the mailing list.

The first teleconference will be held on Friday May 19 at 9:00 GMT.
